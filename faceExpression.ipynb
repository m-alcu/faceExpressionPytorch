{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import torch.utils.data as data\n",
    "from PIL import Image\n",
    "import os\n",
    "import os.path\n",
    "import errno\n",
    "import torch\n",
    "import codecs\n",
    "import numpy as np\n",
    "from random import randint\n",
    "from tqdm import tnrange, tqdm_notebook\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.ndimage as ndi\n",
    "\n",
    "def flatten_matrix(matrix):\n",
    "    vector = matrix.flatten(order='F')\n",
    "    vector = vector.reshape(1, len(vector))\n",
    "    return vector\n",
    "\n",
    "def zca_whitening(inputs):\n",
    "    sigma = np.dot(inputs, inputs.T)/inputs.shape[1] #Correlation matrix\n",
    "    U,S,V = np.linalg.svd(sigma) #Singular Value Decomposition\n",
    "    epsilon = 0.1                #Whitening constant, it prevents division by zero\n",
    "    ZCAMatrix = np.dot(np.dot(U, np.diag(1.0/np.sqrt(np.diag(S) + epsilon))), U.T) #ZCA Whitening matrix\n",
    "    return np.dot(ZCAMatrix, inputs)   #Data whitening\n",
    "\n",
    "def global_contrast_normalize(X, scale=1., subtract_mean=True, use_std=True,\n",
    "                              sqrt_bias=10, min_divisor=1e-8):\n",
    "\n",
    "    \"\"\"\n",
    "    __author__ = \"David Warde-Farley\"\n",
    "    __copyright__ = \"Copyright 2012, Universite de Montreal\"\n",
    "    __credits__ = [\"David Warde-Farley\"]\n",
    "    __license__ = \"3-clause BSD\"\n",
    "    __email__ = \"wardefar@iro\"\n",
    "    __maintainer__ = \"David Warde-Farley\"\n",
    "    .. [1] A. Coates, H. Lee and A. Ng. \"An Analysis of Single-Layer\n",
    "       Networks in Unsupervised Feature Learning\". AISTATS 14, 2011.\n",
    "       http://www.stanford.edu/~acoates/papers/coatesleeng_aistats_2011.pdf\n",
    "    \"\"\"\n",
    "    assert X.ndim == 2, \"X.ndim must be 2\"\n",
    "    scale = float(scale)\n",
    "    assert scale >= min_divisor\n",
    "\n",
    "    mean = X.mean(axis=1)\n",
    "    if subtract_mean:\n",
    "        X = X - mean[:, np.newaxis]  \n",
    "    else:\n",
    "        X = X.copy()\n",
    "    if use_std:\n",
    "        ddof = 1\n",
    "        if X.shape[1] == 1:\n",
    "            ddof = 0\n",
    "        normalizers = np.sqrt(sqrt_bias + X.var(axis=1, ddof=ddof)) / scale\n",
    "    else:\n",
    "        normalizers = np.sqrt(sqrt_bias + (X ** 2).sum(axis=1)) / scale\n",
    "    normalizers[normalizers < min_divisor] = 1.\n",
    "    X /= normalizers[:, np.newaxis]  # Does not make a copy.\n",
    "    return X\n",
    "\n",
    "def ZeroCenter(data):\n",
    "    data = data - np.mean(data,axis=0)\n",
    "    return data\n",
    "\n",
    "def Zerocenter_ZCA_whitening_Global_Contrast_Normalize(list):\n",
    "    Intonumpyarray = np.asarray(list)\n",
    "    data = Intonumpyarray.reshape(48,48)\n",
    "    data2 = ZeroCenter(data)\n",
    "    data3 = zca_whitening(flatten_matrix(data2)).reshape(48,48)\n",
    "    data4 = global_contrast_normalize(data3)\n",
    "    data5 = np.rot90(data4,3)\n",
    "    return data5    \n",
    "\n",
    "\n",
    "def load_data():\n",
    "\n",
    "    train_x = []\n",
    "    train_y = []\n",
    "    val_x =[]\n",
    "    val_y =[]\n",
    "\n",
    "    with open(\"badtrainingdata.txt\", \"r\") as text:\n",
    "        ToBeRemovedTrainingData = []\n",
    "        for line in text:\n",
    "            ToBeRemovedTrainingData.append(int(line))\n",
    "    number = 0\n",
    "\n",
    "    f = open('fer2013.csv')\n",
    "    csv_f = csv.reader(f)\n",
    "\n",
    "    for row in csv_f:   \n",
    "        number+= 1\n",
    "        if number not in ToBeRemovedTrainingData:\n",
    "\n",
    "            if str(row[2]) == \"Training\" or str(row[2]) == \"PublicTest\" :\n",
    "                temp_list = []\n",
    "\n",
    "                for pixel in row[1].split( ):\n",
    "                    temp_list.append(int(pixel))\n",
    "\n",
    "                data = Zerocenter_ZCA_whitening_Global_Contrast_Normalize(temp_list)\n",
    "                train_y.append(int(row[0]))\n",
    "                train_x.append(data.reshape(48,48).tolist())\n",
    "\n",
    "            elif str(row[2]) == \"PrivateTest\":\n",
    "                temp_list = []\n",
    "\n",
    "                for pixel in row[1].split( ):\n",
    "                    temp_list.append(int(pixel))\n",
    "\n",
    "                data = Zerocenter_ZCA_whitening_Global_Contrast_Normalize(temp_list)\n",
    "                val_y.append(int(row[0]))\n",
    "                val_x.append(data.reshape(48,48).tolist())\n",
    "\n",
    "    return train_x, train_y, val_x, val_y\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def apply_transform(x,\n",
    "                    transform_matrix,\n",
    "                    channel_axis=0,\n",
    "                    fill_mode='nearest',\n",
    "                    cval=0.):\n",
    "    \"\"\"Applies the image transformation specified by a matrix.\n",
    "    # Arguments\n",
    "        x: 2D numpy array, single image.\n",
    "        transform_matrix: Numpy array specifying the geometric transformation.\n",
    "        channel_axis: Index of axis for channels in the input tensor.\n",
    "        fill_mode: Points outside the boundaries of the input\n",
    "            are filled according to the given mode\n",
    "            (one of `{'constant', 'nearest', 'reflect', 'wrap'}`).\n",
    "        cval: Value used for points outside the boundaries\n",
    "            of the input if `mode='constant'`.\n",
    "    # Returns\n",
    "        The transformed version of the input.\n",
    "    \"\"\"\n",
    "    x = np.rollaxis(x, channel_axis, 0)\n",
    "    final_affine_matrix = transform_matrix[:2, :2]\n",
    "    final_offset = transform_matrix[:2, 2]\n",
    "    channel_images = [ndi.interpolation.affine_transform(\n",
    "        x_channel,\n",
    "        final_affine_matrix,\n",
    "        final_offset,\n",
    "        order=1,\n",
    "        mode=fill_mode,\n",
    "        cval=cval) for x_channel in x]\n",
    "    x = np.stack(channel_images, axis=0)\n",
    "    x = np.rollaxis(x, 0, channel_axis + 1)\n",
    "    return x\n",
    "\n",
    "def transform_matrix_offset_center(matrix, x, y):\n",
    "    o_x = float(x) / 2 + 0.5\n",
    "    o_y = float(y) / 2 + 0.5\n",
    "    offset_matrix = np.array([[1, 0, o_x], [0, 1, o_y], [0, 0, 1]])\n",
    "    reset_matrix = np.array([[1, 0, -o_x], [0, 1, -o_y], [0, 0, 1]])\n",
    "    transform_matrix = np.dot(np.dot(offset_matrix, matrix), reset_matrix)\n",
    "    return transform_matrix\n",
    "\n",
    "def random_rotation(x, rg, row_axis=1, col_axis=2, channel_axis=0,\n",
    "                    fill_mode='nearest', cval=0.):\n",
    "    \"\"\"Performs a random rotation of a Numpy image tensor.\n",
    "    # Arguments\n",
    "        x: Input tensor. Must be 3D.\n",
    "        rg: Rotation range, in degrees.\n",
    "        row_axis: Index of axis for rows in the input tensor.\n",
    "        col_axis: Index of axis for columns in the input tensor.\n",
    "        channel_axis: Index of axis for channels in the input tensor.\n",
    "        fill_mode: Points outside the boundaries of the input\n",
    "            are filled according to the given mode\n",
    "            (one of `{'constant', 'nearest', 'reflect', 'wrap'}`).\n",
    "        cval: Value used for points outside the boundaries\n",
    "            of the input if `mode='constant'`.\n",
    "    # Returns\n",
    "        Rotated Numpy image tensor.\n",
    "    \"\"\"\n",
    "    theta = np.deg2rad(np.random.uniform(-rg, rg))\n",
    "    rotation_matrix = np.array([[np.cos(theta), -np.sin(theta), 0],\n",
    "                                [np.sin(theta), np.cos(theta), 0],\n",
    "                                [0, 0, 1]])\n",
    "\n",
    "    h, w = x.shape[row_axis], x.shape[col_axis]\n",
    "    transform_matrix = transform_matrix_offset_center(rotation_matrix, h, w)\n",
    "    x = apply_transform(x, transform_matrix, channel_axis, fill_mode, cval)\n",
    "    return x\n",
    "\n",
    "\n",
    "def random_shift(x, wrg, hrg, row_axis=1, col_axis=2, channel_axis=0,\n",
    "                 fill_mode='nearest', cval=0.):\n",
    "    \"\"\"Performs a random spatial shift of a Numpy image tensor.\n",
    "    # Arguments\n",
    "        x: Input tensor. Must be 3D.\n",
    "        wrg: Width shift range, as a float fraction of the width.\n",
    "        hrg: Height shift range, as a float fraction of the height.\n",
    "        row_axis: Index of axis for rows in the input tensor.\n",
    "        col_axis: Index of axis for columns in the input tensor.\n",
    "        channel_axis: Index of axis for channels in the input tensor.\n",
    "        fill_mode: Points outside the boundaries of the input\n",
    "            are filled according to the given mode\n",
    "            (one of `{'constant', 'nearest', 'reflect', 'wrap'}`).\n",
    "        cval: Value used for points outside the boundaries\n",
    "            of the input if `mode='constant'`.\n",
    "    # Returns\n",
    "        Shifted Numpy image tensor.\n",
    "    \"\"\"\n",
    "    h, w = x.shape[row_axis], x.shape[col_axis]\n",
    "    tx = np.random.uniform(-hrg, hrg) * h\n",
    "    ty = np.random.uniform(-wrg, wrg) * w\n",
    "    translation_matrix = np.array([[1, 0, tx],\n",
    "                                   [0, 1, ty],\n",
    "                                   [0, 0, 1]])\n",
    "\n",
    "    transform_matrix = translation_matrix  # no need to do offset\n",
    "    x = apply_transform(x, transform_matrix, channel_axis, fill_mode, cval)\n",
    "    return x\n",
    "\n",
    "def random_vertical_flip(x, p=0.5):\n",
    "    \"\"\"Performs a random Vertical Flip.\n",
    "    # Arguments\n",
    "        x: Input tensor. Must be 3D.\n",
    "    # Returns\n",
    "        Flipped tensor\n",
    "    \"\"\"\n",
    "    sal = np.copy(x)\n",
    "    for i in np.ndindex(x.shape[:1]):\n",
    "        if np.random.random() < p:\n",
    "            sal[i] = flip_axis(sal[i], 1)\n",
    "    return sal\n",
    "\n",
    "def flip_axis(x, axis):\n",
    "    x = np.asarray(x).swapaxes(axis, 0)\n",
    "    x = x[::-1, ...]\n",
    "    x = x.swapaxes(0, axis)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class facialRecognitionDataset(data.Dataset):\n",
    "    \"\"\"facialRecognitionDataset Dataset.\n",
    "\n",
    "    Args:\n",
    "        root (string): Root directory of dataset where ``processed/training.pt``\n",
    "            and  ``processed/test.pt`` exist.\n",
    "        train (bool, optional): If True, creates dataset from ``training.pt``,\n",
    "            otherwise from ``test.pt``.\n",
    "        transform (callable, optional): A function/transform that  takes in an PIL image\n",
    "            and returns a transformed version. E.g, ``transforms.RandomCrop``\n",
    "        target_transform (callable, optional): A function/transform that takes in the\n",
    "            target and transforms it.\n",
    "    \"\"\"\n",
    "    raw_folder = 'raw'\n",
    "    processed_folder = 'processed'\n",
    "    training_file = 'training.pt'\n",
    "    test_file = 'test.pt'\n",
    "\n",
    "    def __init__(self, root, train=True, transform=None, target_transform=None):\n",
    "        self.root = os.path.expanduser(root)\n",
    "        self.train = train  # training set or test set\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        \n",
    "\n",
    "        self.convert_data()\n",
    "        \n",
    "        if not self._check_exists():\n",
    "            raise RuntimeError('Dataset not found.')\n",
    "\n",
    "        if self.train:\n",
    "            self.train_data, self.train_labels = torch.load(\n",
    "                os.path.join(root, self.processed_folder, self.training_file))\n",
    "            \n",
    "            self.random_affine()\n",
    "            self.y_train_tensor = torch.LongTensor(self.train_labels)\n",
    "        else:\n",
    "            self.test_data, self.test_labels = torch.load(os.path.join(root, self.processed_folder, self.test_file))\n",
    "\n",
    "            self.x_test_tensor = torch.FloatTensor(self.test_data).view(-1, 48, 48)\n",
    "            self.y_test_tensor = torch.LongTensor(self.test_labels) \n",
    "\n",
    "        \n",
    "        print(\"init_facial\")\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            index (int): Index\n",
    "\n",
    "        Returns:\n",
    "            tuple: (image, target) where target is index of the target class.\n",
    "        \"\"\"\n",
    "        \n",
    "        if self.train:\n",
    "            img, target = self.x_train_tensor[index], self.y_train_tensor[index]\n",
    "        else:\n",
    "            img, target = self.x_test_tensor[index], self.y_test_tensor[index]\n",
    "\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        if self.target_transform is not None:\n",
    "            target = self.target_transform(target)\n",
    "\n",
    "        img2 = img.unsqueeze(0)\n",
    "            \n",
    "        return img2, target\n",
    "\n",
    "    def __len__(self):\n",
    "        if self.train:\n",
    "            return 32181\n",
    "        else:\n",
    "            return 3589\n",
    "\n",
    "    def _check_exists(self):\n",
    "        return os.path.exists(os.path.join(self.root, self.processed_folder, self.training_file)) and \\\n",
    "            os.path.exists(os.path.join(self.root, self.processed_folder, self.test_file))\n",
    "\n",
    "    def random_affine(self):\n",
    "        self.train_data_random = np.copy(self.train_data)\n",
    "        self.train_data_random = random_rotation(self.train_data_random,5)\n",
    "        self.train_data_random = random_shift(self.train_data_random,0.1,0.1)\n",
    "        self.train_data_random = random_vertical_flip(self.train_data_random)\n",
    "        self.x_train_tensor = torch.FloatTensor(self.train_data_random).view(-1, 48, 48)\n",
    "        \n",
    "        \n",
    "    def convert_data(self):\n",
    "        \"\"\"Convert data files.\"\"\"\n",
    "        \n",
    "        if self._check_exists():\n",
    "            return\n",
    "\n",
    "        # download files\n",
    "        try:\n",
    "            os.makedirs(os.path.join(self.root, self.processed_folder))\n",
    "        except OSError as e:\n",
    "            if e.errno == errno.EEXIST:\n",
    "                pass\n",
    "            else:\n",
    "                raise\n",
    "\n",
    "        # process and save as torch files\n",
    "        print('Processing...')\n",
    "        \n",
    "        x_train, y_train, x_test, y_test = load_data()\n",
    "\n",
    "        #x_train = torch.FloatTensor(X).view(-1, 48, 48)\n",
    "        #y_train = torch.LongTensor(Y)\n",
    "        #x_test = torch.FloatTensor(X_test).view(-1, 48, 48)\n",
    "        #y_test = torch.LongTensor(Y_test)        \n",
    "        \n",
    "        training_set = (\n",
    "            x_train,\n",
    "            y_train\n",
    "        )\n",
    "        test_set = (\n",
    "            x_test,\n",
    "            y_test\n",
    "        )\n",
    "        with open(os.path.join(self.root, self.processed_folder, self.training_file), 'wb') as f:\n",
    "            torch.save(training_set, f)\n",
    "        with open(os.path.join(self.root, self.processed_folder, self.test_file), 'wb') as f:\n",
    "            torch.save(test_set, f)\n",
    "\n",
    "        print('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.autograd import Variable\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init_facial\n",
      "init_facial\n"
     ]
    }
   ],
   "source": [
    "# Training settings\n",
    "batch_size = 64\n",
    "is_cuda = torch.cuda.is_available()\n",
    "\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True} if is_cuda else {}\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    facialRecognitionDataset('../data198', train=True,\n",
    "                   transform=None),\n",
    "    batch_size=batch_size, shuffle=True, **kwargs)\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    facialRecognitionDataset('../data198', train=False,\n",
    "               transform=None),\n",
    "    batch_size=batch_size, shuffle=True, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loader.batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "loaded = False\n",
    "for batch_idx, (data, target) in enumerate(train_loader):\n",
    "    if loaded == False:\n",
    "        data2 = data\n",
    "        target2 = target\n",
    "        loaded = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP4AAAD8CAYAAABXXhlaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nxI5vTIPY8Y1pEDu+MQ1ixzemQf4PM4YvsP+LozAAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(data2[1][0].numpy(), cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "a must be greater than 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-39-87005b2e35ca>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mY2\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mY2\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mN\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0mj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m48\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m48\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmap\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'gray'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_title\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel_map\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mmtrand.pyx\u001b[0m in \u001b[0;36mmtrand.RandomState.choice\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: a must be greater than 0"
     ]
    }
   ],
   "source": [
    "label_map = ['Anger', 'Disgust', 'Fear', 'Happy', 'Sad', 'Surprise', 'Neutral']\n",
    "num_map = len(label_map)\n",
    "X2, Y2 = data2.numpy(), target2.numpy()\n",
    "while True:\n",
    "    fig, axes = plt.subplots(1, num_map, figsize=(16, 32),subplot_kw={'xticks': [], 'yticks': []})\n",
    "    fig.subplots_adjust(hspace=0.3, wspace=0.05)\n",
    "    i=0\n",
    "    for ax, i in zip(axes.flat, np.linspace(0, num_map, num_map, endpoint=False)):\n",
    "        x, y = X2[Y2==i], Y2[Y2==i]\n",
    "        N = len(y)\n",
    "        j = np.random.choice(N)\n",
    "        ax.imshow(x[j][0].reshape(48, 48), cmap='gray')\n",
    "        ax.set_title(label_map[y[j]])\n",
    "    plt.show()\n",
    "    prompt = input('Quit? Enter Y:\\n')\n",
    "    if prompt == 'Y':\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 64, kernel_size=5)\n",
    "        self.mp = nn.MaxPool2d(kernel_size=5, stride=2, padding=2)\n",
    "        self.conv2 = nn.Conv2d(64, 64, kernel_size=3, padding=2)\n",
    "        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, padding=1)\n",
    "        self.avgp1 = nn.AvgPool2d(kernel_size=3, stride=2)\n",
    "        self.conv4 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.conv5 = nn.Conv2d(128, 128, kernel_size=3, padding=1)\n",
    "        self.avgp2 = nn.AvgPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        self.fc1 = nn.Linear(4608, 1024)\n",
    "        self.fc2 = nn.Linear(1024, 1024)\n",
    "        self.fc3 = nn.Linear(1024, 7)\n",
    "        self.drop = nn.Dropout(p=0.2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        in_size = x.size(0)\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(self.mp(x)))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = F.relu(self.conv4(self.avgp1(x)))\n",
    "        x = F.relu(self.conv5(x))\n",
    "        x = F.relu(self.avgp2(x))\n",
    "        x = x.view(in_size, -1)  # flatten the tensor\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(self.drop(x)))\n",
    "        x = F.relu(self.fc3(self.drop(x)))\n",
    "        return F.log_softmax(x,dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Net()\n",
    "if is_cuda:\n",
    "    model.cuda()\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        if is_cuda:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "        data, target = Variable(data), Variable(target)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test():\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    for data, target in test_loader:\n",
    "        if is_cuda:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "        with torch.no_grad():\n",
    "            data, target = Variable(data), Variable(target)\n",
    "        output = model(data)\n",
    "        # sum up batch loss\n",
    "        test_loss += F.nll_loss(output, target, size_average=False).data\n",
    "        # get the index of the max log-probability\n",
    "        pred = output.data.max(1, keepdim=True)[1]\n",
    "        correct += pred.eq(target.data.view_as(pred)).cpu().sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('Test set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d814a49709a94a8f986b9faf7171f71e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='train cnn...', max=80), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-328:\n",
      "Traceback (most recent call last):\n",
      "  File \"/anaconda3/envs/tfdeeplearning/lib/python3.5/threading.py\", line 914, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/anaconda3/envs/tfdeeplearning/lib/python3.5/site-packages/tqdm/_monitor.py\", line 63, in run\n",
      "    for instance in self.tqdm_cls._instances:\n",
      "  File \"/anaconda3/envs/tfdeeplearning/lib/python3.5/_weakrefset.py\", line 60, in __iter__\n",
      "    for itemref in self.data:\n",
      "RuntimeError: Set changed size during iteration\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: Average loss: 1.9459, Accuracy: 491/3589 (14%)\n",
      "Test set: Average loss: 1.9459, Accuracy: 491/3589 (14%)\n",
      "Test set: Average loss: 1.9459, Accuracy: 491/3589 (14%)\n",
      "Test set: Average loss: 1.9106, Accuracy: 879/3589 (24%)\n",
      "Test set: Average loss: 1.8959, Accuracy: 879/3589 (24%)\n",
      "Test set: Average loss: 1.8947, Accuracy: 879/3589 (24%)\n",
      "Test set: Average loss: 1.8946, Accuracy: 879/3589 (24%)\n",
      "Test set: Average loss: 1.8907, Accuracy: 879/3589 (24%)\n",
      "Test set: Average loss: 1.8899, Accuracy: 879/3589 (24%)\n",
      "Test set: Average loss: 1.8898, Accuracy: 879/3589 (24%)\n",
      "Test set: Average loss: 1.8898, Accuracy: 879/3589 (24%)\n",
      "Test set: Average loss: 1.8899, Accuracy: 879/3589 (24%)\n",
      "Test set: Average loss: 1.8898, Accuracy: 879/3589 (24%)\n",
      "Test set: Average loss: 1.8898, Accuracy: 879/3589 (24%)\n",
      "Test set: Average loss: 1.8898, Accuracy: 879/3589 (24%)\n",
      "Test set: Average loss: 1.8900, Accuracy: 879/3589 (24%)\n",
      "Test set: Average loss: 1.8896, Accuracy: 879/3589 (24%)\n",
      "Test set: Average loss: 1.8898, Accuracy: 879/3589 (24%)\n",
      "Test set: Average loss: 1.8896, Accuracy: 879/3589 (24%)\n",
      "Test set: Average loss: 1.8897, Accuracy: 879/3589 (24%)\n",
      "Test set: Average loss: 1.8896, Accuracy: 879/3589 (24%)\n",
      "Test set: Average loss: 1.8894, Accuracy: 879/3589 (24%)\n",
      "Test set: Average loss: 1.8892, Accuracy: 879/3589 (24%)\n",
      "Test set: Average loss: 1.8875, Accuracy: 879/3589 (24%)\n",
      "Test set: Average loss: 1.8864, Accuracy: 879/3589 (24%)\n",
      "Test set: Average loss: 1.8836, Accuracy: 879/3589 (24%)\n",
      "Test set: Average loss: 1.8794, Accuracy: 879/3589 (24%)\n",
      "Test set: Average loss: 1.8123, Accuracy: 922/3589 (26%)\n",
      "Test set: Average loss: 1.7731, Accuracy: 947/3589 (26%)\n",
      "Test set: Average loss: 1.7840, Accuracy: 968/3589 (27%)\n",
      "Test set: Average loss: 1.7371, Accuracy: 1126/3589 (31%)\n",
      "Test set: Average loss: 1.6702, Accuracy: 1225/3589 (34%)\n",
      "Test set: Average loss: 1.6749, Accuracy: 1233/3589 (34%)\n",
      "Test set: Average loss: 1.6846, Accuracy: 1177/3589 (33%)\n",
      "Test set: Average loss: 1.6052, Accuracy: 1309/3589 (36%)\n",
      "Test set: Average loss: 1.6416, Accuracy: 1263/3589 (35%)\n",
      "Test set: Average loss: 1.5847, Accuracy: 1366/3589 (38%)\n",
      "Test set: Average loss: 1.5590, Accuracy: 1420/3589 (40%)\n",
      "Test set: Average loss: 1.5123, Accuracy: 1492/3589 (42%)\n",
      "Test set: Average loss: 1.4711, Accuracy: 1519/3589 (42%)\n",
      "Test set: Average loss: 1.5003, Accuracy: 1473/3589 (41%)\n",
      "Test set: Average loss: 1.3929, Accuracy: 1623/3589 (45%)\n",
      "Test set: Average loss: 1.5089, Accuracy: 1423/3589 (40%)\n",
      "Test set: Average loss: 1.3635, Accuracy: 1675/3589 (47%)\n",
      "Test set: Average loss: 1.3348, Accuracy: 1746/3589 (49%)\n",
      "Test set: Average loss: 1.3272, Accuracy: 1714/3589 (48%)\n",
      "Test set: Average loss: 1.3876, Accuracy: 1667/3589 (46%)\n",
      "Test set: Average loss: 1.3628, Accuracy: 1657/3589 (46%)\n",
      "Test set: Average loss: 1.3034, Accuracy: 1767/3589 (49%)\n",
      "Test set: Average loss: 1.2758, Accuracy: 1801/3589 (50%)\n",
      "Test set: Average loss: 1.3147, Accuracy: 1777/3589 (50%)\n",
      "Test set: Average loss: 1.3441, Accuracy: 1761/3589 (49%)\n",
      "Test set: Average loss: 1.2953, Accuracy: 1786/3589 (50%)\n",
      "Test set: Average loss: 1.2916, Accuracy: 1809/3589 (50%)\n",
      "Test set: Average loss: 1.2162, Accuracy: 1934/3589 (54%)\n",
      "Test set: Average loss: 1.3328, Accuracy: 1772/3589 (49%)\n",
      "Test set: Average loss: 1.1814, Accuracy: 1995/3589 (56%)\n",
      "Test set: Average loss: 1.2957, Accuracy: 1850/3589 (52%)\n",
      "Test set: Average loss: 1.2992, Accuracy: 1807/3589 (50%)\n",
      "Test set: Average loss: 1.2462, Accuracy: 1970/3589 (55%)\n",
      "Test set: Average loss: 1.3123, Accuracy: 1856/3589 (52%)\n",
      "Test set: Average loss: 1.1463, Accuracy: 2050/3589 (57%)\n",
      "Test set: Average loss: 1.1994, Accuracy: 1975/3589 (55%)\n",
      "Test set: Average loss: 1.2957, Accuracy: 1873/3589 (52%)\n",
      "Test set: Average loss: 1.2165, Accuracy: 1928/3589 (54%)\n",
      "Test set: Average loss: 1.2315, Accuracy: 1937/3589 (54%)\n",
      "Test set: Average loss: 1.1542, Accuracy: 2041/3589 (57%)\n",
      "Test set: Average loss: 1.1944, Accuracy: 1988/3589 (55%)\n",
      "Test set: Average loss: 1.1225, Accuracy: 2076/3589 (58%)\n",
      "Test set: Average loss: 1.1377, Accuracy: 2094/3589 (58%)\n",
      "Test set: Average loss: 1.1677, Accuracy: 2021/3589 (56%)\n",
      "Test set: Average loss: 1.1463, Accuracy: 2087/3589 (58%)\n",
      "Test set: Average loss: 1.1621, Accuracy: 2019/3589 (56%)\n",
      "Test set: Average loss: 1.1498, Accuracy: 2103/3589 (59%)\n",
      "Test set: Average loss: 1.1588, Accuracy: 2043/3589 (57%)\n",
      "Test set: Average loss: 1.1116, Accuracy: 2110/3589 (59%)\n",
      "Test set: Average loss: 1.1564, Accuracy: 2091/3589 (58%)\n",
      "Test set: Average loss: 1.0851, Accuracy: 2190/3589 (61%)\n",
      "Test set: Average loss: 1.1140, Accuracy: 2144/3589 (60%)\n",
      "Test set: Average loss: 1.1609, Accuracy: 2104/3589 (59%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "for epoch in tqdm_notebook(range(80), desc='train cnn...'):\n",
    "    train(epoch)\n",
    "    train_loader.dataset.random_affine()\n",
    "    test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
